{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Latihan Praktis Advanced\n",
    "\n",
    "Notebook ini berisi latihan praktis komprehensif untuk memahami dan menguasai XGBoost.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "Setelah menyelesaikan latihan ini, Anda akan dapat:\n",
    "1. Menggunakan XGBoost untuk classification dan regression\n",
    "2. Melakukan hyperparameter tuning yang efektif\n",
    "3. Menginterpretasi model dengan feature importance dan SHAP\n",
    "4. Menangani overfitting dengan regularization dan early stopping\n",
    "5. Membandingkan XGBoost dengan algoritma lain\n",
    "6. Mengoptimalkan performa untuk kompetisi ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup dan Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style untuk plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 1: XGBoost Classification dari Dasar\n",
    "\n",
    "**Tugas**: Implementasikan XGBoost classifier dengan breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# TODO: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Your code here\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create dan train XGBoost classifier\n",
    "xgb_clf = XGBClassifier(\n",
    "    # Your hyperparameters here\n",
    "    # Tip: start with default parameters\n",
    ")\n",
    "\n",
    "# TODO: Fit model dengan evaluation set untuk monitoring\n",
    "xgb_clf.fit(\n",
    "    # Your code here\n",
    "    # Don't forget eval_set and early_stopping_rounds\n",
    ")\n",
    "\n",
    "# TODO: Make predictions\n",
    "y_pred = # Your code here\n",
    "y_pred_proba = # Your code here\n",
    "\n",
    "# TODO: Calculate metrics\n",
    "accuracy = # Your code here\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': # Your code here\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# TODO: Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Your plotting code here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot learning curve dari evals_result\n",
    "results = xgb_clf.evals_result()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Training vs Validation Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs = range(len(results['validation_0']['logloss']))\n",
    "# Your plotting code here\n",
    "\n",
    "# Plot 2: Overfitting Analysis\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calculate and plot training-validation gap\n",
    "# Your code here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 2: Hyperparameter Tuning\n",
    "\n",
    "**Tugas**: Lakukan hyperparameter tuning untuk meningkatkan performa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    # Add more parameters\n",
    "}\n",
    "\n",
    "# TODO: Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    # Your code here\n",
    ")\n",
    "\n",
    "# TODO: Fit grid search\n",
    "print(\"üîç Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚ú® Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"üèÜ Best CV score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare default vs tuned model\n",
    "default_model = XGBClassifier(random_state=42)\n",
    "tuned_model = grid_search.best_estimator_\n",
    "\n",
    "# Train both models\n",
    "# Your code here\n",
    "\n",
    "# Compare performance\n",
    "default_score = # Your code here\n",
    "tuned_score = # Your code here\n",
    "\n",
    "print(f\"Default model accuracy: {default_score:.4f}\")\n",
    "print(f\"Tuned model accuracy: {tuned_score:.4f}\")\n",
    "print(f\"Improvement: {tuned_score - default_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 3: XGBoost Regression\n",
    "\n",
    "**Tugas**: Gunakan XGBoost untuk regression task dengan California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_reg, y_reg = housing.data, housing.target\n",
    "\n",
    "print(f\"Housing dataset shape: {X_reg.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target statistics:\")\n",
    "print(f\"  Mean: ${y_reg.mean():.2f} (hundreds of thousands)\")\n",
    "print(f\"  Std: ${y_reg.std():.2f}\")\n",
    "print(f\"  Range: ${y_reg.min():.2f} - ${y_reg.max():.2f}\")\n",
    "\n",
    "# TODO: Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    # Your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create dan train XGBoost regressor\n",
    "xgb_reg = XGBRegressor(\n",
    "    # Your hyperparameters here\n",
    ")\n",
    "\n",
    "# TODO: Train with early stopping\n",
    "xgb_reg.fit(\n",
    "    # Your code here\n",
    ")\n",
    "\n",
    "# TODO: Make predictions and evaluate\n",
    "y_pred_reg = # Your code here\n",
    "\n",
    "# Calculate regression metrics\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Regression Results:\")\n",
    "print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create diagnostic plots untuk regression\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Predictions vs Actual\n",
    "# Your plotting code here\n",
    "\n",
    "# 2. Residuals plot\n",
    "# Your plotting code here\n",
    "\n",
    "# 3. Feature importance\n",
    "# Your plotting code here\n",
    "\n",
    "# 4. Learning curve\n",
    "# Your plotting code here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 4: Cross-Validation dan Model Validation\n",
    "\n",
    "**Tugas**: Implementasikan robust model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform k-fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Setup cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# TODO: Cross-validate XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    # Your code here\n",
    ")\n",
    "\n",
    "print(f\"Cross-Validation Results:\")\n",
    "print(f\"  Scores: {cv_scores}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std: {cv_scores.std():.4f}\")\n",
    "print(f\"  95% CI: {cv_scores.mean() - 2*cv_scores.std():.4f} - {cv_scores.mean() + 2*cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare dengan XGBoost native cross-validation\n",
    "# Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# TODO: XGBoost native CV\n",
    "cv_results = xgb.cv(\n",
    "    # Your parameters here\n",
    ")\n",
    "\n",
    "print(f\"XGBoost Native CV Results:\")\n",
    "print(f\"  Best iteration: {len(cv_results)}\")\n",
    "print(f\"  Best train score: {cv_results['train-logloss-mean'].iloc[-1]:.4f}\")\n",
    "print(f\"  Best test score: {cv_results['test-logloss-mean'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 5: Model Comparison\n",
    "\n",
    "**Tugas**: Bandingkan XGBoost dengan algoritma ensemble lainnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup multiple models for comparison\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    # Add more models if you want\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# TODO: Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Cross-validate\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test Accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize model comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: CV Scores\n",
    "plt.subplot(1, 2, 1)\n",
    "# Your plotting code here\n",
    "\n",
    "# Plot 2: Test Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "# Your plotting code here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 6: Advanced - Handling Imbalanced Data\n",
    "\n",
    "**Tugas**: Gunakan XGBoost untuk imbalanced classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_imbal, y_imbal = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Imbalanced dataset:\")\n",
    "print(f\"  Total samples: {len(y_imbal)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_imbal)}\")\n",
    "print(f\"  Class ratio: {np.bincount(y_imbal)[1]/np.bincount(y_imbal)[0]:.3f}\")\n",
    "\n",
    "X_train_imbal, X_test_imbal, y_train_imbal, y_test_imbal = train_test_split(\n",
    "    X_imbal, y_imbal, test_size=0.2, random_state=42, stratify=y_imbal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train XGBoost with scale_pos_weight\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train_imbal == 0).sum() / (y_train_imbal == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Models to compare\n",
    "models_imbal = {\n",
    "    'XGBoost (default)': XGBClassifier(random_state=42),\n",
    "    'XGBoost (balanced)': XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost (class_weight)': XGBClassifier(\n",
    "        # Use class_weight parameter if available\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# TODO: Train and evaluate models\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "imbal_results = {}\n",
    "\n",
    "for name, model in models_imbal.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model.fit(X_train_imbal, y_train_imbal)\n",
    "    y_pred = model.predict(X_test_imbal)\n",
    "    y_pred_proba = model.predict_proba(X_test_imbal)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_imbal, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test_imbal, y_pred, average='binary'\n",
    "    )\n",
    "    auc = roc_auc_score(y_test_imbal, y_pred_proba)\n",
    "    \n",
    "    imbal_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "\n",
    "# Display comparison\n",
    "imbal_df = pd.DataFrame(imbal_results).T\n",
    "print(\"\\nImbalanced Data Results:\")\n",
    "print(imbal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Latihan 7: Feature Engineering untuk XGBoost\n",
    "\n",
    "**Tugas**: Advanced feature engineering techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create feature interactions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Using housing dataset\n",
    "housing_df = pd.DataFrame(X_reg, columns=housing.feature_names)\n",
    "housing_df['target'] = y_reg\n",
    "\n",
    "print(\"Original features:\")\n",
    "print(housing_df.columns.tolist())\n",
    "\n",
    "# TODO: Create domain-specific features\n",
    "# Rooms per household\n",
    "housing_df['rooms_per_household'] = housing_df['AveRooms'] / housing_df['AveOccup']\n",
    "\n",
    "# Bedrooms ratio\n",
    "housing_df['bedrooms_ratio'] = housing_df['AveBedrms'] / housing_df['AveRooms']\n",
    "\n",
    "# Population per household\n",
    "housing_df['pop_per_household'] = housing_df['Population'] / housing_df['HouseholdIncome']\n",
    "\n",
    "# TODO: Add your own feature engineering ideas\n",
    "# Your code here\n",
    "\n",
    "print(f\"\\nAfter feature engineering: {len(housing_df.columns)-1} features\")\n",
    "print(\"New features created:\")\n",
    "new_features = [col for col in housing_df.columns if col not in housing.feature_names and col != 'target']\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare original vs engineered features\n",
    "X_original = housing_df[housing.feature_names]\n",
    "X_engineered = housing_df.drop('target', axis=1)\n",
    "y_target = housing_df['target']\n",
    "\n",
    "# Split both datasets\n",
    "X_orig_train, X_orig_test, y_orig_train, y_orig_test = train_test_split(\n",
    "    X_original, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
    "    X_engineered, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train models\n",
    "model_original = XGBRegressor(n_estimators=100, random_state=42)\n",
    "model_engineered = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# TODO: Fit and evaluate both models\n",
    "# Your code here\n",
    "\n",
    "# Compare results\n",
    "print(\"Feature Engineering Impact:\")\n",
    "print(f\"  Original features R¬≤: {r2_original:.4f}\")\n",
    "print(f\"  Engineered features R¬≤: {r2_engineered:.4f}\")\n",
    "print(f\"  Improvement: {r2_engineered - r2_original:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Challenge: Kaggle-Style Competition\n",
    "\n",
    "**Tugas**: Kombinasikan semua teknik untuk mencapai performa terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your best XGBoost model\n",
    "# Combine all techniques you've learned:\n",
    "# 1. Feature engineering\n",
    "# 2. Hyperparameter tuning  \n",
    "# 3. Cross-validation\n",
    "# 4. Early stopping\n",
    "# 5. Regularization\n",
    "\n",
    "def build_ultimate_xgboost_model(X, y, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Build the ultimate XGBoost model with all best practices\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# TODO: Test your ultimate model\n",
    "# Use breast cancer dataset for final challenge\n",
    "ultimate_model = build_ultimate_xgboost_model(X, y, 'classification')\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "final_cv_scores = cross_val_score(ultimate_model, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(f\"üèÜ Ultimate XGBoost Model Results:\")\n",
    "print(f\"  CV Accuracy: {final_cv_scores.mean():.4f} ¬± {final_cv_scores.std():.4f}\")\n",
    "print(f\"  CV Scores: {final_cv_scores}\")\n",
    "\n",
    "# Compare with baseline\n",
    "baseline = XGBClassifier(random_state=42)\n",
    "baseline_scores = cross_val_score(baseline, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nüìä Baseline vs Ultimate:\")\n",
    "print(f\"  Baseline: {baseline_scores.mean():.4f} ¬± {baseline_scores.std():.4f}\")\n",
    "print(f\"  Ultimate: {final_cv_scores.mean():.4f} ¬± {final_cv_scores.std():.4f}\")\n",
    "print(f\"  Improvement: {final_cv_scores.mean() - baseline_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Reflection dan Best Practices\n",
    "\n",
    "**Tugas**: Analisis results dan identifikasi best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive analysis\n",
    "print(\"üéì XGBoost Learning Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Summarize all your findings\n",
    "print(\"\"\"\n",
    "Key Insights from Exercises:\n",
    "\n",
    "1. Feature Importance:\n",
    "   - Most important features for breast cancer: [Your findings]\n",
    "   - Most important features for housing prices: [Your findings]\n",
    "\n",
    "2. Hyperparameter Impact:\n",
    "   - Most influential parameters: [Your findings]\n",
    "   - Best parameter combinations: [Your findings]\n",
    "\n",
    "3. Model Performance:\n",
    "   - XGBoost vs Random Forest: [Your findings]\n",
    "   - XGBoost vs Gradient Boosting: [Your findings]\n",
    "\n",
    "4. Advanced Techniques:\n",
    "   - Feature engineering impact: [Your findings]\n",
    "   - Cross-validation insights: [Your findings]\n",
    "   - Imbalanced data handling: [Your findings]\n",
    "\n",
    "5. Best Practices Discovered:\n",
    "   - [Your best practice 1]\n",
    "   - [Your best practice 2]\n",
    "   - [Your best practice 3]\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Create your XGBoost checklist\n",
    "xgboost_checklist = \"\"\"\n",
    "üîß XGBoost Implementation Checklist:\n",
    "\n",
    "Data Preparation:\n",
    "‚ñ° Handle missing values (XGBoost can handle them, but preprocessing might help)\n",
    "‚ñ° Encode categorical variables\n",
    "‚ñ° Feature scaling (optional but can help)\n",
    "‚ñ° Create domain-specific features\n",
    "\n",
    "Model Training:\n",
    "‚ñ° Set appropriate objective function\n",
    "‚ñ° Use eval_set for monitoring\n",
    "‚ñ° Set early_stopping_rounds\n",
    "‚ñ° Start with default parameters\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "‚ñ° Tune n_estimators with early stopping\n",
    "‚ñ° Adjust learning_rate and max_depth\n",
    "‚ñ° Try different subsample values\n",
    "‚ñ° Apply regularization (reg_alpha, reg_lambda)\n",
    "\n",
    "Model Validation:\n",
    "‚ñ° Use cross-validation for robust estimates\n",
    "‚ñ° Plot learning curves\n",
    "‚ñ° Check for overfitting\n",
    "‚ñ° Validate on holdout test set\n",
    "\n",
    "Model Interpretation:\n",
    "‚ñ° Analyze feature importance\n",
    "‚ñ° Use SHAP for detailed interpretation\n",
    "‚ñ° Check partial dependence plots\n",
    "‚ñ° Validate model makes sense\n",
    "\"\"\"\n",
    "\n",
    "print(xgboost_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps dan Advanced Topics\n",
    "\n",
    "Setelah menyelesaikan latihan ini, explore topik advanced berikut:\n",
    "\n",
    "### üöÄ Advanced XGBoost:\n",
    "1. **Multi-class classification** dengan objective='multi:softprob'\n",
    "2. **Custom objective functions** dan evaluation metrics\n",
    "3. **GPU acceleration** dengan tree_method='gpu_hist'\n",
    "4. **Incremental learning** dengan xgb.train()\n",
    "\n",
    "### üîÑ Ensemble Methods:\n",
    "1. **Stacking** XGBoost dengan algoritma lain\n",
    "2. **Blending** multiple XGBoost models\n",
    "3. **Bayesian optimization** untuk hyperparameter tuning\n",
    "4. **AutoML** dengan XGBoost backend\n",
    "\n",
    "### üìä Production Deployment:\n",
    "1. **Model serialization** dan versioning\n",
    "2. **Real-time prediction** APIs\n",
    "3. **Model monitoring** dan drift detection\n",
    "4. **A/B testing** untuk model comparison\n",
    "\n",
    "### üèÜ Competition Techniques:\n",
    "1. **Feature selection** dengan recursive elimination\n",
    "2. **Target encoding** untuk categorical features\n",
    "3. **Pseudo-labeling** untuk semi-supervised learning\n",
    "4. **Multi-level modeling** dan meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**üéâ Selamat!** Anda telah menyelesaikan comprehensive XGBoost tutorial! \n",
    "\n",
    "**Next Algorithm**: Explore **LightGBM** dan **CatBoost** untuk alternative gradient boosting implementations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}