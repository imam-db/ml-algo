# XGBoost (eXtreme Gradient Boosting)

**üè† [Back to Main](../../README.md)** | **üìÅ [Advanced Topics](../README.md)** | **‚¨ÖÔ∏è [Previous: Model Evaluation](../03_model_evaluation/README.md)**

## üîë Key Terms
**Quick definitions for this algorithm** - üìö [Full Glossary](../../GLOSSARY.md)

- **[XGBoost](../../GLOSSARY.md#boosting)** - eXtreme Gradient Boosting, advanced ensemble method
- **[Ensemble Method](../../GLOSSARY.md#ensemble-method)** - Combining multiple models for better performance
- **[Boosting](../../GLOSSARY.md#boosting)** - Sequential training where each model corrects previous errors
- **[Hyperparameter Tuning](../../GLOSSARY.md#hyperparameter-tuning)** - Finding optimal algorithm configuration settings
- **[Cross-Validation](../../GLOSSARY.md#cross-validation-cv)** - Technique to assess model performance robustly
- **[Feature Importance](../../GLOSSARY.md#feature-importance)** - Measure of each feature's contribution to predictions
- **[SHAP](../../GLOSSARY.md#shap-shapley-additive-explanations)** - Method to explain individual predictions

## üó∫Ô∏è Quick Access

| File | Description | Quick Run |
|------|-------------|----------|
| [`implementation.py`](./implementation.py) | Basic XGBoost usage | `uv run python implementation.py` |
| [`sklearn_example.py`](./sklearn_example.py) | Comprehensive examples | `uv run python sklearn_example.py` |
| [`hyperparameter_tuning.py`](./hyperparameter_tuning.py) | Advanced tuning | `uv run python hyperparameter_tuning.py` |
| [`exercise.ipynb`](./exercise.ipynb) | Interactive exercises | `uv run jupyter lab exercise.ipynb` |

## üìñ Theory and Concepts

### What is XGBoost?
[**XGBoost**](../../GLOSSARY.md#boosting) (eXtreme Gradient Boosting) is an advanced implementation of [**gradient boosting**](../../GLOSSARY.md#boosting) optimized for speed and performance. XGBoost is one of the most popular and successful [**algorithms**](../../GLOSSARY.md#algorithm) in machine learning competitions, especially for tabular data.

### Gradient Boosting Concept
[**Gradient Boosting**](../../GLOSSARY.md#boosting) is an [**ensemble method**](../../GLOSSARY.md#ensemble-method) that:
1. Builds [**models**](../../GLOSSARY.md#model) sequentially
2. Each new model tries to correct the errors of previous models
3. Uses [**gradient descent**](../../GLOSSARY.md#gradient-descent) to minimize the [**loss function**](../../GLOSSARY.md#cost-function-loss-function)

### Mathematical Formula
**Objective Function:**
```
Obj(Œ∏) = Œ£ L(yi, ≈∑i) + Œ£ Œ©(fk)
```

Where:
- `L(yi, ≈∑i)` = Loss function (training loss)
- `Œ©(fk)` = Regularization term
- `≈∑i = Œ£ fk(xi)` = Prediction (sum of trees)

**XGBoost Innovation:**
- **Second-order approximation** of loss function
- **Regularization** terms in objective
- **Optimized tree construction** algorithm

## üéØ Kapan Menggunakan XGBoost?

### ‚úÖ Sangat Cocok untuk:
- **Data tabular/structured data** 
- **Kompetisi machine learning** (Kaggle, etc.)
- **Data dengan missing values**
- **Mixed data types** (numerical + categorical)
- **Classification dan Regression tasks**
- **Feature importance** analysis
- **Dataset berukuran menengah-besar**

### ‚ùå Tidak ideal untuk:
- **Computer vision** (CNN lebih baik)
- **Natural Language Processing** (Transformer lebih baik)  
- **Dataset yang sangat kecil** (< 1000 samples)
- **Real-time prediction** (latency tinggi)
- **Data dengan struktur temporal kompleks**

## üîç Kelebihan dan Kekurangan

### Kelebihan:
- ‚úÖ **Performa excellent** pada data tabular
- ‚úÖ **Handle missing values** secara native
- ‚úÖ **Built-in regularization** (L1, L2)
- ‚úÖ **Feature importance** yang akurat
- ‚úÖ **Parallel processing** dan GPU support
- ‚úÖ **Cross-validation** built-in
- ‚úÖ **Early stopping** untuk mencegah overfitting
- ‚úÖ **Robust terhadap outliers**

### Kekurangan:
- ‚ùå **Hyperparameter tuning** yang kompleks
- ‚ùå **Memory intensive** untuk dataset besar
- ‚ùå **Prone to overfitting** jika tidak di-tune dengan baik
- ‚ùå **Black box model** (sulit interpretasi)
- ‚ùå **Training time** relatif lama
- ‚ùå **Sensitive to noisy data**

## ‚öôÔ∏è Hyperparameters Utama

### Tree-specific Parameters:
- **n_estimators**: Jumlah trees (default: 100)
- **max_depth**: Kedalaman maksimum tree (default: 6)
- **min_child_weight**: Minimum sum of weights di leaf (default: 1)
- **subsample**: Fraction of samples untuk setiap tree (default: 1)
- **colsample_bytree**: Fraction of features untuk setiap tree (default: 1)

### Learning Parameters:
- **learning_rate/eta**: Step size shrinkage (default: 0.3)
- **objective**: Loss function (reg:squarederror, binary:logistic, etc.)
- **eval_metric**: Evaluation metric (rmse, logloss, auc, etc.)

### Regularization Parameters:
- **reg_alpha**: L1 regularization (default: 0)
- **reg_lambda**: L2 regularization (default: 1)
- **gamma**: Minimum loss reduction untuk split (default: 0)

## üìä XGBoost vs Other Algorithms

| Aspek | XGBoost | Random Forest | Gradient Boosting | Linear Models |
|-------|---------|---------------|-------------------|---------------|
| **Accuracy** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Speed** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Interpretability** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Overfitting Risk** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Hyperparameter Tuning** | ‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## üõ†Ô∏è Instalasi dan Setup

### Instalasi XGBoost:
```bash
# Via pip
pip install xgboost

# Via conda
conda install -c conda-forge xgboost

# Via UV (recommended untuk project ini)
uv add xgboost
```

### GPU Support (Optional):
```bash
# For CUDA support
pip install xgboost[gpu]
```

## üìà Evaluation dan Model Selection

### Classification Metrics:
- **Accuracy**: Overall correct predictions
- **Precision/Recall**: Class-specific performance
- **F1-Score**: Harmonic mean of precision/recall
- **AUC-ROC**: Area under ROC curve
- **Log Loss**: Probabilistic loss

### Regression Metrics:
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error
- **R¬≤**: Coefficient of determination
- **MAPE**: Mean Absolute Percentage Error

### Cross-Validation:
XGBoost memiliki built-in CV:
```python
cv_results = xgb.cv(
    params, dtrain, num_boost_round=100,
    nfold=5, early_stopping_rounds=10
)
```

## üéì Best Practices

### 1. **Data Preparation**:
- Handle missing values (XGBoost can handle, but preprocessing might help)
- Encode categorical variables (LabelEncoder atau One-hot)
- Feature scaling tidak wajib, tapi bisa membantu

### 2. **Hyperparameter Tuning**:
- Start dengan default parameters
- Tune **n_estimators** dengan early stopping
- Adjust **learning_rate** dan **max_depth**
- Use **RandomizedSearchCV** atau **Optuna**

### 3. **Overfitting Prevention**:
- Use **early_stopping_rounds**
- Apply **regularization** (reg_alpha, reg_lambda)
- Reduce **learning_rate** dan increase **n_estimators**
- Use **cross-validation** untuk monitoring

### 4. **Feature Engineering**:
- Create interaction features
- Polynomial features untuk non-linear relationships
- Domain-specific feature engineering

### 5. **Model Interpretation**:
- Plot **feature importance**
- Use **SHAP** values untuk detailed interpretation
- Analyze **learning curves**

## üìä Workflow Tipikal

1. **Data Preparation** ‚Üí Clean, encode, split
2. **Baseline Model** ‚Üí Default parameters
3. **Hyperparameter Tuning** ‚Üí GridSearch/RandomSearch
4. **Cross-Validation** ‚Üí Robust performance estimate  
5. **Feature Importance** ‚Üí Understanding model
6. **Model Interpretation** ‚Üí SHAP, partial dependence
7. **Final Evaluation** ‚Üí Test set performance

## üéØ Real-World Scenarios

### üèÜ Scenario 1: Kaggle Competition Style
**Dataset**: Tabular competition data (Titanic, House Prices)
**Goal**: Maximize leaderboard score
**Run**: `uv run python sklearn_example.py --scenario kaggle`

**What you'll master:**
- Feature engineering techniques
- Cross-validation strategies
- Ensemble methods with multiple models
- Hyperparameter optimization

### üí≥ Scenario 2: Credit Risk Assessment
**Dataset**: Customer financial data
**Goal**: Predict loan default probability
**Run**: `uv run python sklearn_example.py --scenario credit_risk`

**What you'll master:**
- Handling imbalanced datasets
- Feature importance analysis
- Business-oriented metrics (precision/recall)
- Model interpretability with SHAP

### üè≠ Scenario 3: E-commerce Sales Forecasting
**Dataset**: Historical sales, seasonal data
**Goal**: Predict monthly sales volume
**Run**: `uv run python sklearn_example.py --scenario sales_forecast`

**What you'll master:**
- Time series feature engineering
- Seasonal decomposition
- Multi-step forecasting
- Business impact evaluation

### üé≠ Scenario 4: Customer Churn Prediction
**Dataset**: Customer behavior, usage patterns
**Goal**: Identify customers likely to churn
**Run**: `uv run python sklearn_example.py --scenario churn`

**What you'll master:**
- Behavioral feature engineering
- Cost-sensitive learning
- Actionable insights generation
- Model monitoring strategies

### üèè Scenario 5: Medical Diagnosis Support
**Dataset**: Patient symptoms, medical history
**Goal**: Assist in disease diagnosis
**Run**: `uv run python sklearn_example.py --scenario medical`

**What you'll master:**
- High-stakes prediction scenarios
- Confidence interval estimation
- False positive/negative trade-offs
- Regulatory compliance considerations

### üîç Hyperparameter Tuning Scenarios

#### Quick Tuning (5-10 minutes)
```bash
uv run python hyperparameter_tuning.py --method random_search --budget small
```

#### Production Tuning (30-60 minutes)
```bash
uv run python hyperparameter_tuning.py --method optuna --budget production
```

#### Competition Tuning (2-4 hours)
```bash
uv run python hyperparameter_tuning.py --method bayesian --budget competition
```

### [`implementation.py`](./implementation.py)
- **Basic XGBoost workflows** for beginners
- **Quick start examples** for each scenario
- **Baseline model setup**
- **Performance comparison** with other algorithms

### [`sklearn_example.py`](./sklearn_example.py)
- **Production-ready implementations** for all 5 scenarios
- **Advanced preprocessing** pipelines
- **Feature engineering** automation
- **Model evaluation** and **business metrics**
- **SHAP interpretability** integration

### [`hyperparameter_tuning.py`](./hyperparameter_tuning.py)
- **Multiple tuning strategies**: Random Search, Bayesian, Optuna
- **Budget-aware optimization**
- **Parallel processing** for faster tuning
- **Competition-grade** parameter spaces

### [`exercise.ipynb`](./exercise.ipynb)
- **Interactive step-by-step** tutorials
- **Real dataset exploration** and **EDA**
- **Hyperparameter sensitivity** analysis
- **Model interpretation** exercises

## üéì Learning Progression

### Prerequisites
- **Machine Learning Basics**: [Linear Regression](../../01_supervised_learning/01_linear_regression/), [Decision Trees](../../01_supervised_learning/03_decision_trees/)
- **Ensemble Methods**: [Random Forest](../../01_supervised_learning/04_random_forest/)
- **Model Evaluation**: [Evaluation Techniques](../03_model_evaluation/)

### Recommended Learning Path
1. **Start** with [`implementation.py`](./implementation.py) - Basic concepts
2. **Practice** with [`exercise.ipynb`](./exercise.ipynb) - Interactive learning
3. **Explore** [`sklearn_example.py`](./sklearn_example.py) - Real scenarios
4. **Master** [`hyperparameter_tuning.py`](./hyperparameter_tuning.py) - Advanced techniques

## üèÜ Competition Tips

1. **Feature Engineering** is king
2. **Ensemble** with different seeds
3. **Blend** XGBoost with LightGBM/CatBoost
4. **Pseudo-labeling** for semi-supervised learning
5. **Target encoding** for high-cardinality categorical
6. **Stratified K-Fold** for robust CV

### Related Advanced Algorithms

| Algorithm | Comparison | Use Case |
|-----------|------------|----------|
| [**LightGBM**](../05_lightgbm/) | Faster training | Large datasets |
| [**CatBoost**](../06_catboost/) | Better categorical handling | Mixed data types |
| [**Random Forest**](../../01_supervised_learning/04_random_forest/) | More interpretable | Feature importance |
| [**Neural Networks**](../../01_supervised_learning/08_neural_networks/) | More flexible | Complex patterns |

## üìö References

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [XGBoost Paper](https://arxiv.org/abs/1603.02754)
- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/index.html)
- [Interpretable ML - XGBoost](https://christophm.github.io/interpretable-ml-book/)

---
**‚û°Ô∏è Next Step**: After mastering XGBoost, explore **LightGBM** and **CatBoost** for even more powerful ensemble methods!
