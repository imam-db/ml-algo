# Linear Regression

## ğŸ“– Theory and Concepts

### What is Linear Regression?
Linear Regression is a supervised learning algorithm used to predict target values (continuous) based on one or more input features. This algorithm finds the best linear relationship between input and output.

### Mathematical Formula
**Simple Linear Regression (1 variable):**
```
y = Î²â‚€ + Î²â‚x + Îµ
```

**Multiple Linear Regression (multiple variables):**
```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ
```

Where:
- `y` = target variable (dependent)
- `x` = feature variable (independent) 
- `Î²â‚€` = intercept (bias)
- `Î²â‚, Î²â‚‚, ..., Î²â‚™` = coefficients (slope)
- `Îµ` = error/noise

### Cost Function
Linear regression menggunakan **Mean Squared Error (MSE)** sebagai cost function:
```
MSE = (1/n) * Î£(yi - Å·i)Â²
```

### Optimization
To find the best parameters, you can use:
1. **Normal Equation** (Closed-form solution)
2. **Gradient Descent** (Iterative approach)

## ğŸ¯ When to Use Linear Regression?

### âœ… Suitable for:
- Predicting continuous values (house prices, temperature, salary, etc.)
- Data with linear relationships
- Model interpretation that is easy to understand
- Baseline model for comparison
- Small to medium-sized datasets

### âŒ Not suitable for:
- Data with complex non-linear relationships
- Many outliers
- Categorical target variables
- High multicollinearity

## ğŸ“Š Linear Regression Assumptions

1. **Linearity**: Linear relationship between X and y
2. **Independence**: Observations are independent
3. **Homoscedasticity**: Constant error variance
4. **Normal Distribution**: Errors are normally distributed
5. **No Multicollinearity**: Features are not highly correlated

## ğŸ“ˆ Model Evaluation

### Metrics for Regression:
- **RÂ² (Coefficient of Determination)**: Proportion of variance explained
- **MSE (Mean Squared Error)**: Average squared error
- **RMSE (Root Mean Squared Error)**: Square root of MSE
- **MAE (Mean Absolute Error)**: Average absolute error

## ğŸ” Advantages and Disadvantages

### Advantages:
- âœ… Simple and fast
- âœ… Easy to interpret
- âœ… No complex parameter tuning required
- âœ… Good baseline
- âœ… Not prone to overfitting

### Disadvantages:
- âŒ Can only capture linear relationships
- âŒ Sensitive to outliers
- âŒ Strict assumptions
- âŒ Poor performance on non-linear data

## ğŸ“ Implementation

In this folder, you will find:
- `implementation.py` - From-scratch implementation using NumPy
- `sklearn_example.py` - Examples using scikit-learn
- `exercise.ipynb` - Practical exercises with real datasets

## ğŸ“ Tips for Practice

1. **Always visualize data** before modeling
2. **Check assumptions** of linear regression
3. **Handle outliers** if necessary
4. **Feature scaling** for multiple regression
5. **Validate model** using cross-validation

## ğŸ“š References

- [Scikit-learn Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)
- [Introduction to Statistical Learning - Chapter 3](https://www.statlearning.com/)

---
**Next Step**: After understanding Linear Regression, continue to **Logistic Regression** for classification problems!
