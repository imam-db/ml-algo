# Linear Regression

**ğŸ  [Back to Main](../../README.md)** | **ğŸ“ [Supervised Learning](../README.md)** | **â¡ï¸ [Next: Logistic Regression](../02_logistic_regression/README.md)**

## ğŸ”‘ Key Terms
**Quick definitions for this algorithm** - ğŸ“š [Full Glossary](../../GLOSSARY.md)

- **[Linear Regression](../../GLOSSARY.md#linear-model)** - Algorithm assuming linear relationship between features and target
- **[Overfitting](../../GLOSSARY.md#overfitting)** - When model learns training data too well, performs poorly on new data  
- **[MSE](../../GLOSSARY.md#mse-mean-squared-error)** - Mean Squared Error, measures average squared prediction errors
- **[RÂ²](../../GLOSSARY.md#rÂ²-r-squared--coefficient-of-determination)** - Proportion of variance explained by model (0-1, higher better)
- **[Gradient Descent](../../GLOSSARY.md#gradient-descent)** - Optimization algorithm that minimizes cost function iteratively
- **[Normal Equation](../../GLOSSARY.md#normal-equation)** - Analytical solution: Î¸ = (X^T Ã— X)^(-1) Ã— X^T Ã— y

## ğŸ—ºï¸ Quick Access

| File | Description | Quick Run |
|------|-------------|----------|
| [`implementation.py`](./implementation.py) | From-scratch implementation | `uv run python implementation.py` |
| [`sklearn_example.py`](./sklearn_example.py) | Scikit-learn examples | `uv run python sklearn_example.py` |
| [`exercise.ipynb`](./exercise.ipynb) | Interactive exercises (Bahasa Indonesia) | `uv run jupyter lab exercise.ipynb` |
| [`exercise_en.ipynb`](./exercise_en.ipynb) | Interactive exercises (English) | `uv run jupyter lab exercise_en.ipynb` |
| [`exercise_EN.md`](./exercise_EN.md) | Exercises (English, Markdown) | Open and follow instructions |

## ğŸ“– Theory and Concepts

### What is Linear Regression?
[**Linear Regression**](../../GLOSSARY.md#linear-model) is a [**supervised learning**](../../GLOSSARY.md#supervised-learning) [**algorithm**](../../GLOSSARY.md#algorithm) used to predict [**target values**](../../GLOSSARY.md#target-variable) (continuous) based on one or more input [**features**](../../GLOSSARY.md#feature). This algorithm finds the best linear relationship between input and output.

### Mathematical Formula
**Simple Linear Regression (1 variable):**
```
y = Î²â‚€ + Î²â‚x + Îµ
```

**Multiple Linear Regression (multiple variables):**
```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ
```

Where:
- `y` = target variable (dependent)
- `x` = feature variable (independent) 
- `Î²â‚€` = intercept (bias)
- `Î²â‚, Î²â‚‚, ..., Î²â‚™` = coefficients (slope)
- `Îµ` = error/noise

### Cost Function
Linear regression menggunakan **Mean Squared Error (MSE)** sebagai cost function:
```
MSE = (1/n) * Î£(yi - Å·i)Â²
```

### Optimization
To find the best parameters, you can use:
1. **Normal Equation** (Closed-form solution)
2. **Gradient Descent** (Iterative approach)

## ğŸ¯ When to Use Linear Regression?

### âœ… Suitable for:
- Predicting continuous values (house prices, temperature, salary, etc.)
- Data with linear relationships
- Model interpretation that is easy to understand
- Baseline model for comparison
- Small to medium-sized datasets

### âŒ Not suitable for:
- Data with complex non-linear relationships
- Many outliers
- Categorical target variables
- High multicollinearity

## ğŸ“Š Linear Regression Assumptions

1. **Linearity**: Linear relationship between X and y
2. **Independence**: Observations are independent
3. **Homoscedasticity**: Constant error variance
4. **Normal Distribution**: Errors are normally distributed
5. **No Multicollinearity**: Features are not highly correlated

## ğŸ“ˆ Model Evaluation

### Metrics for Regression:
- **RÂ² (Coefficient of Determination)**: Proportion of variance explained
- **MSE (Mean Squared Error)**: Average squared error
- **RMSE (Root Mean Squared Error)**: Square root of MSE
- **MAE (Mean Absolute Error)**: Average absolute error

## ğŸ” Advantages and Disadvantages

### Advantages:
- âœ… Simple and fast
- âœ… Easy to interpret
- âœ… No complex parameter tuning required
- âœ… Good baseline
- âœ… Not prone to overfitting

### Disadvantages:
- âŒ Can only capture linear relationships
- âŒ Sensitive to outliers
- âŒ Strict assumptions
- âŒ Poor performance on non-linear data

## ğŸ¯ Practical Scenarios

### ğŸ  Scenario 1: House Price Prediction
**Dataset**: House features (size, bedrooms, location)
**Goal**: Predict house prices
**Run**: `uv run python sklearn_example.py --scenario house_prices`

**What you'll learn:**
- Feature scaling for multiple variables
- Handling categorical variables
- Model interpretation with coefficients

### ğŸŒ¡ï¸ Scenario 2: Temperature Prediction
**Dataset**: Weather features (humidity, pressure, wind speed)
**Goal**: Predict daily temperature
**Run**: `uv run python sklearn_example.py --scenario temperature`

**What you'll learn:**
- Time series data preparation
- Feature correlation analysis
- Seasonal trend modeling

### ğŸ’° Scenario 3: Salary Estimation
**Dataset**: Employee data (experience, education, skills)
**Goal**: Estimate salary ranges
**Run**: `uv run python sklearn_example.py --scenario salary`

**What you'll learn:**
- Polynomial features
- Cross-validation techniques
- Confidence intervals

### ğŸ“ˆ Scenario 4: Stock Price Trend
**Dataset**: Historical stock data
**Goal**: Simple trend analysis
**Run**: `uv run python implementation.py --scenario stocks`

**What you'll learn:**
- Gradient descent vs Normal equation
- Learning rate optimization
- Cost function visualization

### ğŸ”¬ Compare Methods
**Compare Normal Equation vs Gradient Descent:**
```bash
uv run python implementation.py --compare-methods
```

## ğŸ“ Implementation Files

### [`implementation.py`](./implementation.py)
- **LinearRegressionFromScratch** class
- **Two optimization methods**: Normal Equation & Gradient Descent
- **Built-in scenarios**: All 4 practical examples above
- **Visualization tools**: Cost history, prediction plots

### [`sklearn_example.py`](./sklearn_example.py)
- **Production-ready examples** using scikit-learn
- **Advanced techniques**: Polynomial features, regularization
- **Model evaluation**: RÂ², RMSE, MAE metrics
- **Cross-validation** and **hyperparameter tuning**

### [`exercise.ipynb`](./exercise.ipynb)
- **Interactive Jupyter notebook**
- **Step-by-step tutorials** for each scenario
- **Visualization exercises**
- **Real dataset exploration**

## ğŸ“ Tips for Practice

1. **Always visualize data** before modeling
2. **Check assumptions** of linear regression
3. **Handle outliers** if necessary
4. **Feature scaling** for multiple regression
5. **Validate model** using cross-validation

## ğŸ“š References

- [Scikit-learn Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)
- [Introduction to Statistical Learning - Chapter 3](https://www.statlearning.com/)

## ğŸ“ Learning Progression

### Prerequisites
- Basic Python knowledge
- NumPy fundamentals ([NumPy Tutorial](https://numpy.org/doc/stable/user/quickstart.html))
- Basic statistics (mean, variance)

### After Linear Regression, explore:

1. **[Logistic Regression](../02_logistic_regression/)** - Classification extension
2. **[Polynomial Features](../../06_utils/)** - Non-linear relationships  
3. **[Model Evaluation](../../04_advanced_topics/03_model_evaluation/)** - Advanced metrics
4. **[Regularization](../05_svm/)** - Ridge/Lasso regression

### Related Algorithms

| Algorithm | Similarity | When to Use |
|-----------|------------|-------------|
| [**Logistic Regression**](../02_logistic_regression/) | Same math, different output | Classification problems |
| [**SVM Regression**](../05_svm/) | Linear boundary | Robust to outliers |
| [**Neural Networks**](../08_neural_networks/) | Extension | Complex non-linear patterns |
| [**Random Forest**](../04_random_forest/) | Ensemble approach | Feature importance |

---
**â¡ï¸ Next Step**: Continue to **[Logistic Regression](../02_logistic_regression/)** for classification problems!
