{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Hands-On Exercises üå≤\n",
    "\n",
    "Welcome to the Random Forest interactive exercises! This notebook contains practical exercises to help you master Random Forest concepts, implementation, and applications.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By completing these exercises, you will:\n",
    "- Understand Random Forest algorithm components\n",
    "- Compare custom implementation with scikit-learn\n",
    "- Master hyperparameter tuning techniques\n",
    "- Handle different types of datasets\n",
    "- Interpret feature importance\n",
    "- Apply Random Forest to real-world problems\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Basic understanding of decision trees\n",
    "- Familiarity with Python and pandas\n",
    "- Knowledge of machine learning concepts\n",
    "\n",
    "## üóÇÔ∏è Exercise Structure\n",
    "1. **Warm-up**: Basic concepts and implementation understanding\n",
    "2. **Implementation**: Build components from scratch\n",
    "3. **Comparison**: Custom vs. scikit-learn implementation\n",
    "4. **Hyperparameter Tuning**: Optimize model performance\n",
    "5. **Real-world Applications**: Solve practical problems\n",
    "6. **Advanced Topics**: Feature importance, ensemble analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, roc_curve\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Ready to start Random Forest exercises!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 1: Understanding Random Forest Components üå±\n",
    "\n",
    "Let's start by understanding the fundamental components of Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bootstrap Sampling\n",
    "\n",
    "**Task**: Implement bootstrap sampling and analyze the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y, random_state=None):\n",
    "    \"\"\"\n",
    "    Create a bootstrap sample from the dataset.\n",
    "    \n",
    "    TODO: Implement bootstrap sampling\n",
    "    - Sample n instances WITH replacement from X and y\n",
    "    - Return bootstrap samples and out-of-bag indices\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X_bootstrap, y_bootstrap, oob_indices\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # TODO: Implement bootstrap sampling\n",
    "    # Hint: Use np.random.choice with replace=True\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y_test = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "X_boot, y_boot, oob_idx = bootstrap_sample(X_test, y_test, random_state=42)\n",
    "print(f\"Original indices: {list(range(len(X_test)))}\")\n",
    "print(f\"OOB indices: {oob_idx}\")\n",
    "print(f\"Bootstrap sample shape: {X_boot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output Analysis**:\n",
    "- Approximately 63.2% of original samples should appear in bootstrap sample\n",
    "- Remaining 36.8% should be out-of-bag (OOB)\n",
    "- Some samples may appear multiple times in bootstrap sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bootstrap sampling properties\n",
    "def analyze_bootstrap_properties(n_samples=1000, n_trials=1000):\n",
    "    \"\"\"\n",
    "    Analyze the statistical properties of bootstrap sampling.\n",
    "    \n",
    "    TODO: Complete this analysis\n",
    "    \"\"\"\n",
    "    X_dummy = np.arange(n_samples).reshape(-1, 1)\n",
    "    y_dummy = np.zeros(n_samples)\n",
    "    \n",
    "    oob_sizes = []\n",
    "    unique_samples = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # TODO: Generate bootstrap sample and calculate statistics\n",
    "        # Calculate:\n",
    "        # 1. Number of OOB samples\n",
    "        # 2. Number of unique samples in bootstrap\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Average OOB percentage: {np.mean(oob_sizes) / n_samples * 100:.2f}%\")\n",
    "    print(f\"Expected OOB percentage: {(1 - 1/np.e) * 100:.2f}%\")\n",
    "    print(f\"Average unique samples: {np.mean(unique_samples):.1f}\")\n",
    "    print(f\"Expected unique samples: {n_samples * (1 - (1 - 1/n_samples)**n_samples):.1f}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_bootstrap_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Feature Selection\n",
    "\n",
    "**Task**: Implement random feature selection for tree splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_features(n_features, max_features='sqrt', random_state=None):\n",
    "    \"\"\"\n",
    "    Select random subset of features for splitting.\n",
    "    \n",
    "    TODO: Implement random feature selection\n",
    "    \n",
    "    Args:\n",
    "        n_features: Total number of features\n",
    "        max_features: Number or strategy for feature selection\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Array of selected feature indices\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # TODO: Implement feature selection logic\n",
    "    # Handle different max_features strategies:\n",
    "    # - 'sqrt': sqrt(n_features)\n",
    "    # - 'log2': log2(n_features)\n",
    "    # - int: exact number\n",
    "    # - None: all features\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test feature selection\n",
    "print(\"Feature selection tests:\")\n",
    "for strategy in ['sqrt', 'log2', 3, None]:\n",
    "    features = get_random_features(16, strategy, random_state=42)\n",
    "    print(f\"{strategy}: {len(features) if features is not None else 'None'} features selected\")\n",
    "    if features is not None and len(features) <= 8:\n",
    "        print(f\"  Indices: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gini Impurity and Information Gain\n",
    "\n",
    "**Task**: Implement impurity measures used in Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity.\n",
    "    \n",
    "    TODO: Implement Gini impurity calculation\n",
    "    Formula: Gini = 1 - Œ£(p_i^2) where p_i is proportion of class i\n",
    "    \n",
    "    Args:\n",
    "        y: Array of class labels\n",
    "    \n",
    "    Returns:\n",
    "        Gini impurity value\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # TODO: Calculate Gini impurity\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate entropy.\n",
    "    \n",
    "    TODO: Implement entropy calculation\n",
    "    Formula: Entropy = -Œ£(p_i * log2(p_i))\n",
    "    \n",
    "    Args:\n",
    "        y: Array of class labels\n",
    "    \n",
    "    Returns:\n",
    "        Entropy value\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # TODO: Calculate entropy\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test impurity measures\n",
    "test_cases = [\n",
    "    [0, 0, 0, 0],  # Pure\n",
    "    [0, 1, 0, 1],  # Balanced\n",
    "    [0, 0, 0, 1],  # Imbalanced\n",
    "    [0, 1, 2, 0, 1, 2]  # Multi-class\n",
    "]\n",
    "\n",
    "print(\"Impurity Measure Tests:\")\n",
    "print(\"Labels\\t\\tGini\\tEntropy\")\n",
    "print(\"-\" * 35)\n",
    "for labels in test_cases:\n",
    "    gini = gini_impurity(labels)\n",
    "    ent = entropy(labels)\n",
    "    print(f\"{labels}\\t{gini:.3f}\\t{ent:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2: Building a Simple Random Forest üõ†Ô∏è\n",
    "\n",
    "Now let's build a simplified Random Forest from the components we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRandomForest:\n",
    "    \"\"\"\n",
    "    Simplified Random Forest implementation for educational purposes.\n",
    "    \n",
    "    TODO: Complete the implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=10, max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Random Forest.\n",
    "        \n",
    "        TODO: Implement training logic\n",
    "        1. Create bootstrap samples\n",
    "        2. Train decision trees with random features\n",
    "        3. Store trained trees\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.trees = []\n",
    "        \n",
    "        # TODO: Train n_estimators decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Create bootstrap sample\n",
    "            # Train decision tree with max_features\n",
    "            # Store tree\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using majority voting.\n",
    "        \n",
    "        TODO: Implement prediction logic\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # TODO: Collect predictions from all trees\n",
    "        # Use majority voting for final prediction\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "# Test your Simple Random Forest\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and test\n",
    "srf = SimpleRandomForest(n_estimators=10, random_state=42)\n",
    "srf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = srf.score(X_train, y_train)\n",
    "test_acc = srf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Simple Random Forest Results:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 3: Comparison with Scikit-Learn üìä\n",
    "\n",
    "Compare your implementation with scikit-learn's Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_implementations(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare Simple Random Forest with sklearn Random Forest.\n",
    "    \n",
    "    TODO: Complete the comparison\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Simple Random Forest\n",
    "    srf = SimpleRandomForest(n_estimators=50, random_state=random_state)\n",
    "    # TODO: Train and evaluate Simple Random Forest\n",
    "    \n",
    "    # Scikit-learn Random Forest\n",
    "    sklearn_rf = RandomForestClassifier(n_estimators=50, random_state=random_state)\n",
    "    # TODO: Train and evaluate Scikit-learn Random Forest\n",
    "    \n",
    "    # Single Decision Tree for comparison\n",
    "    dt = DecisionTreeClassifier(random_state=random_state)\n",
    "    # TODO: Train and evaluate Decision Tree\n",
    "    \n",
    "    # Print results\n",
    "    results = {\n",
    "        'Simple Random Forest': (0, 0),  # Replace with actual results\n",
    "        'Scikit-learn RF': (0, 0),       # Replace with actual results  \n",
    "        'Single Decision Tree': (0, 0)    # Replace with actual results\n",
    "    }\n",
    "    \n",
    "    print(\"Model Comparison Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Model':<20} {'Train Acc':<12} {'Test Acc':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, (train_acc, test_acc) in results.items():\n",
    "        print(f\"{model_name:<20} {train_acc:<12.4f} {test_acc:<12.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison on different datasets\n",
    "print(\"=== Iris Dataset ===\")\n",
    "iris_results = compare_implementations(iris.data, iris.target)\n",
    "\n",
    "print(\"\\n=== Wine Dataset ===\")\n",
    "wine = load_wine()\n",
    "wine_results = compare_implementations(wine.data, wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Ensemble Effect Analysis\n",
    "\n",
    "**Task**: Analyze how the number of trees affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ensemble_effect(X, y, max_estimators=100, step=10):\n",
    "    \"\"\"\n",
    "    Analyze how the number of estimators affects Random Forest performance.\n",
    "    \n",
    "    TODO: Complete this analysis\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    estimator_range = range(1, max_estimators + 1, step)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    # TODO: Train Random Forest with different numbers of estimators\n",
    "    # Record training and test accuracies\n",
    "    \n",
    "    for n_est in estimator_range:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(estimator_range, train_scores, label='Training Accuracy', marker='o')\n",
    "    plt.plot(estimator_range, test_scores, label='Test Accuracy', marker='s')\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Random Forest Performance vs Number of Trees')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return estimator_range, train_scores, test_scores\n",
    "\n",
    "# Analyze ensemble effect\n",
    "est_range, train_scores, test_scores = analyze_ensemble_effect(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 4: Hyperparameter Tuning üéõÔ∏è\n",
    "\n",
    "Master the art of Random Forest hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Grid Search Optimization\n",
    "\n",
    "**Task**: Implement comprehensive hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Perform comprehensive hyperparameter tuning for Random Forest.\n",
    "    \n",
    "    TODO: Complete the hyperparameter tuning\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        # TODO: Define parameter grid for tuning\n",
    "        # Include: n_estimators, max_depth, min_samples_split, \n",
    "        #          min_samples_leaf, max_features\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement Grid Search\n",
    "    # Use 5-fold cross-validation\n",
    "    # Optimize for accuracy\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Evaluate best model\n",
    "    # TODO: Get best model and evaluate on test set\n",
    "    \n",
    "    print(\"Hyperparameter Tuning Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    # Print best parameters and scores\n",
    "    \n",
    "    return None  # Return grid search object\n",
    "\n",
    "# Create a more complex dataset for tuning\n",
    "X_complex, y_complex = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best_rf = hyperparameter_optimization(X_complex, y_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Importance Analysis\n",
    "\n",
    "**Task**: Analyze and visualize feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using Random Forest.\n",
    "    \n",
    "    TODO: Complete feature importance analysis\n",
    "    \"\"\"\n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = rf.feature_importances_\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # TODO: Create feature importance DataFrame\n",
    "    # Sort by importance\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Create visualizations\n",
    "    # 1. Bar plot of top 10 features\n",
    "    # 2. Feature importance distribution\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return rf, None  # Return rf and importance DataFrame\n",
    "\n",
    "# Analyze feature importance on iris dataset\n",
    "rf_iris, importance_df = analyze_feature_importance(\n",
    "    iris.data, iris.target, iris.feature_names\n",
    ")\n",
    "\n",
    "# Analyze on wine dataset\n",
    "rf_wine, _ = analyze_feature_importance(\n",
    "    wine.data, wine.target, wine.feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 5: Real-World Applications üåç\n",
    "\n",
    "Apply Random Forest to practical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Imbalanced Classification\n",
    "\n",
    "**Task**: Handle imbalanced datasets with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalanced_data():\n",
    "    \"\"\"\n",
    "    Demonstrate handling imbalanced datasets with Random Forest.\n",
    "    \n",
    "    TODO: Complete imbalanced data handling\n",
    "    \"\"\"\n",
    "    # Create imbalanced dataset\n",
    "    X_imb, y_imb = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=10,\n",
    "        n_classes=2,\n",
    "        weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Class distribution: {Counter(y_imb)}\")\n",
    "    print(f\"Imbalance ratio: {Counter(y_imb)[0] / Counter(y_imb)[1]:.1f}:1\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    "    )\n",
    "    \n",
    "    # TODO: Train different Random Forest models:\n",
    "    # 1. Default Random Forest\n",
    "    # 2. Balanced Random Forest (class_weight='balanced')\n",
    "    # 3. Balanced subsample Random Forest\n",
    "    \n",
    "    models = {\n",
    "        'Default RF': None,  # TODO: Create model\n",
    "        'Balanced RF': None,  # TODO: Create model  \n",
    "        'Balanced Subsample RF': None  # TODO: Create model\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if model is not None:\n",
    "            # TODO: Train model and evaluate\n",
    "            # Calculate: accuracy, precision, recall, F1-score, AUC\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "    \n",
    "    # TODO: Create comparison visualization\n",
    "    # ROC curves for all models\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Handle imbalanced data\n",
    "imbalanced_results = handle_imbalanced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Regression with Random Forest\n",
    "\n",
    "**Task**: Apply Random Forest to regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_analysis():\n",
    "    \"\"\"\n",
    "    Demonstrate Random Forest regression.\n",
    "    \n",
    "    TODO: Complete regression analysis\n",
    "    \"\"\"\n",
    "    # Create regression dataset\n",
    "    X_reg, y_reg = make_regression(\n",
    "        n_samples=500,\n",
    "        n_features=10,\n",
    "        n_informative=7,\n",
    "        noise=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reg, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: Train Random Forest Regressor\n",
    "    # Compare with single decision tree\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Evaluate models\n",
    "    # Calculate: MSE, RMSE, R¬≤, MAE\n",
    "    \n",
    "    # TODO: Create visualizations\n",
    "    # 1. Predictions vs Actual values scatter plot\n",
    "    # 2. Residual plot\n",
    "    # 3. Feature importance\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run regression analysis\n",
    "regression_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 6: Advanced Topics üöÄ\n",
    "\n",
    "Explore advanced Random Forest concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Out-of-Bag (OOB) Error Analysis\n",
    "\n",
    "**Task**: Implement and analyze OOB error estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_analysis(X, y):\n",
    "    \"\"\"\n",
    "    Analyze Out-of-Bag error estimation.\n",
    "    \n",
    "    TODO: Complete OOB analysis\n",
    "    \"\"\"\n",
    "    # Split data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Train Random Forest with OOB scoring\n",
    "    # Compare OOB score with validation score\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Analyze OOB score vs number of estimators\n",
    "    estimator_range = range(10, 201, 10)\n",
    "    oob_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for n_est in estimator_range:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # Plot OOB vs validation scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(estimator_range, oob_scores, label='OOB Score', marker='o')\n",
    "    plt.plot(estimator_range, val_scores, label='Validation Score', marker='s')\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title('OOB Score vs Validation Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(oob_scores, val_scores)[0, 1]\n",
    "    print(f\"Correlation between OOB and Validation scores: {correlation:.4f}\")\n",
    "    \n",
    "    return oob_scores, val_scores\n",
    "\n",
    "# Run OOB analysis\n",
    "oob_scores, val_scores = oob_analysis(wine.data, wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Feature Selection with Random Forest\n",
    "\n",
    "**Task**: Use Random Forest for automated feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_pipeline(X, y):\n",
    "    \"\"\"\n",
    "    Implement feature selection using Random Forest importance.\n",
    "    \n",
    "    TODO: Complete feature selection pipeline\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Original dataset: {X.shape[1]} features\")\n",
    "    \n",
    "    # TODO: Method 1 - SelectFromModel with threshold\n",
    "    # Use Random Forest to select important features\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Method 2 - Recursive Feature Elimination\n",
    "    # Use RFECV for optimal number of features\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compare performance:\n",
    "    # 1. All features\n",
    "    # 2. SelectFromModel features\n",
    "    # 3. RFECV features\n",
    "    \n",
    "    results = {\n",
    "        'All Features': None,\n",
    "        'SelectFromModel': None,\n",
    "        'RFECV': None\n",
    "    }\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nFeature Selection Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for method, (n_features, accuracy) in results.items():\n",
    "        if accuracy is not None:\n",
    "            print(f\"{method:<20}: {n_features:>3} features, {accuracy:.4f} accuracy\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create high-dimensional dataset\n",
    "X_high_dim, y_high_dim = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=50,\n",
    "    n_informative=10,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run feature selection\n",
    "fs_results = feature_selection_pipeline(X_high_dim, y_high_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 7: Model Interpretability üîç\n",
    "\n",
    "Understand and interpret Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Partial Dependence Analysis\n",
    "\n",
    "**Task**: Analyze partial dependence of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_dependence_analysis(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Analyze partial dependence of features in Random Forest.\n",
    "    \n",
    "    TODO: Implement partial dependence analysis\n",
    "    \"\"\"\n",
    "    from sklearn.inspection import plot_partial_dependence\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # TODO: Create partial dependence plots\n",
    "    # Select top 4 most important features\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return rf\n",
    "\n",
    "# Analyze partial dependence on iris dataset\n",
    "pd_rf = partial_dependence_analysis(iris.data, iris.target, iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Tree Visualization\n",
    "\n",
    "**Task**: Visualize individual trees from the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trees(X, y, feature_names=None, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualize individual trees from Random Forest.\n",
    "    \n",
    "    TODO: Implement tree visualization\n",
    "    \"\"\"\n",
    "    from sklearn.tree import plot_tree\n",
    "    \n",
    "    # Train a small Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=3,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # TODO: Visualize first 3 trees\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 7))\n",
    "    \n",
    "    for i in range(3):\n",
    "        # YOUR CODE HERE\n",
    "        # Use plot_tree to visualize rf.estimators_[i]\n",
    "        pass\n",
    "    \n",
    "    plt.suptitle('Individual Trees in Random Forest', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return rf\n",
    "\n",
    "# Visualize trees on iris dataset (simplified)\n",
    "tree_rf = visualize_trees(\n",
    "    iris.data, iris.target, \n",
    "    iris.feature_names, iris.target_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ Final Challenge: Complete Random Forest Project\n",
    "\n",
    "Put together everything you've learned in a comprehensive project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Predict Customer Churn\n",
    "\n",
    "**Task**: Build a complete machine learning pipeline using Random Forest to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_churn_project():\n",
    "    \"\"\"\n",
    "    Complete Random Forest project for customer churn prediction.\n",
    "    \n",
    "    TODO: Implement the complete pipeline\n",
    "    \n",
    "    Pipeline steps:\n",
    "    1. Data generation and exploration\n",
    "    2. Data preprocessing\n",
    "    3. Feature engineering\n",
    "    4. Model training and tuning\n",
    "    5. Model evaluation\n",
    "    6. Feature importance analysis\n",
    "    7. Model interpretation\n",
    "    8. Final recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ FINAL PROJECT: Customer Churn Prediction with Random Forest\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Generate synthetic customer data\n",
    "    print(\"üìä Step 1: Data Generation\")\n",
    "    # TODO: Create realistic customer churn dataset\n",
    "    # Include features like: age, tenure, monthly_charges, total_charges, etc.\n",
    "    \n",
    "    # Step 2: Data Exploration\n",
    "    print(\"\\nüîç Step 2: Data Exploration\")\n",
    "    # TODO: Explore data distribution, correlations, class balance\n",
    "    \n",
    "    # Step 3: Data Preprocessing\n",
    "    print(\"\\nüõ†Ô∏è Step 3: Data Preprocessing\")\n",
    "    # TODO: Handle missing values, encode categorical variables, scale features\n",
    "    \n",
    "    # Step 4: Feature Engineering\n",
    "    print(\"\\n‚öôÔ∏è Step 4: Feature Engineering\")\n",
    "    # TODO: Create new features, polynomial features, interaction terms\n",
    "    \n",
    "    # Step 5: Model Training and Tuning\n",
    "    print(\"\\nüéõÔ∏è Step 5: Model Training and Hyperparameter Tuning\")\n",
    "    # TODO: Train Random Forest, optimize hyperparameters\n",
    "    \n",
    "    # Step 6: Model Evaluation\n",
    "    print(\"\\nüìà Step 6: Model Evaluation\")\n",
    "    # TODO: Comprehensive evaluation with multiple metrics\n",
    "    \n",
    "    # Step 7: Feature Importance\n",
    "    print(\"\\nüéØ Step 7: Feature Importance Analysis\")\n",
    "    # TODO: Analyze which features are most important for churn prediction\n",
    "    \n",
    "    # Step 8: Model Interpretation\n",
    "    print(\"\\nüîç Step 8: Model Interpretation\")\n",
    "    # TODO: Partial dependence, SHAP values, business insights\n",
    "    \n",
    "    # Step 9: Final Recommendations\n",
    "    print(\"\\nüí° Step 9: Business Recommendations\")\n",
    "    # TODO: Provide actionable insights based on model findings\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéâ PROJECT COMPLETED! üéâ\")\n",
    "    print(\"Congratulations on completing the Random Forest exercises!\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run the final project\n",
    "customer_churn_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìö Summary and Next Steps\n",
    "\n",
    "## What You've Learned:\n",
    "‚úÖ Random Forest algorithm components and theory  \n",
    "‚úÖ Bootstrap sampling and random feature selection  \n",
    "‚úÖ Building Random Forest from scratch  \n",
    "‚úÖ Comparing implementations with scikit-learn  \n",
    "‚úÖ Hyperparameter tuning strategies  \n",
    "‚úÖ Feature importance analysis  \n",
    "‚úÖ Handling imbalanced datasets  \n",
    "‚úÖ Model interpretation techniques  \n",
    "‚úÖ Real-world application development  \n",
    "\n",
    "## Next Steps:\n",
    "1. **Explore Gradient Boosting**: Learn about XGBoost, LightGBM, CatBoost\n",
    "2. **Study Ensemble Methods**: Bagging, boosting, stacking\n",
    "3. **Advanced Interpretability**: SHAP, LIME, permutation importance\n",
    "4. **Production Deployment**: Model serving, monitoring, maintenance\n",
    "5. **Specialized Applications**: Time series, NLP, computer vision\n",
    "\n",
    "## Resources for Further Learning:\n",
    "- **Books**: \"The Elements of Statistical Learning\" by Hastie et al.\n",
    "- **Courses**: Andrew Ng's Machine Learning Course\n",
    "- **Documentation**: Scikit-learn Random Forest documentation\n",
    "- **Practice**: Kaggle competitions and datasets\n",
    "\n",
    "---\n",
    "\n",
    "**üéä Congratulations on completing the Random Forest exercises!**  \n",
    "You now have a solid understanding of one of the most powerful and widely-used machine learning algorithms. Keep practicing and applying these concepts to real-world problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}