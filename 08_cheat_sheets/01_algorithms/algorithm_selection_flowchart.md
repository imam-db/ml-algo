# ğŸ§­ Algorithm Selection Flowchart

**ğŸ  [Cheat Sheets Home](../README.md)** | **ğŸ [Scikit-Learn Quick Reference](../02_python_sklearn/)** | **ğŸ¯ [Model Evaluation](../04_model_evaluation/)**

---

## ğŸ¯ Quick Summary

Visual decision tree to choose the right machine learning algorithm based on your data size, problem type, and requirements.

---

## ğŸ“Š Quick Decision Tree

```
ğŸ“Š What type of problem do you have?
â”œâ”€ ğŸ·ï¸ CLASSIFICATION (Predicting categories)
â”‚  â”œâ”€ Size < 100K samples?
â”‚  â”‚  â”œâ”€ Linear separable data? â†’ ğŸ“ˆ **Naive Bayes**, **Logistic Regression**
â”‚  â”‚  â”œâ”€ Need interpretability? â†’ ğŸŒ³ **Decision Tree**
â”‚  â”‚  â””â”€ Complex patterns? â†’ ğŸ” **SVM**, **KNN**
â”‚  â””â”€ Size > 100K samples?
â”‚     â”œâ”€ Need speed? â†’ ğŸ“ˆ **Logistic Regression**, **Linear SVM**
â”‚     â”œâ”€ Best performance? â†’ ğŸŒ² **Random Forest**, **Gradient Boosting**
â”‚     â””â”€ Deep patterns? â†’ ğŸ§  **Neural Networks**
â”‚
â”œâ”€ ğŸ“ REGRESSION (Predicting numbers)  
â”‚  â”œâ”€ Linear relationship? â†’ ğŸ“ˆ **Linear Regression**
â”‚  â”œâ”€ Non-linear patterns? â†’ ğŸŒ² **Random Forest**, **SVR**
â”‚  â”œâ”€ Need interpretability? â†’ ğŸŒ³ **Decision Tree**
â”‚  â””â”€ High dimensions? â†’ ğŸ” **Ridge**, **Lasso**
â”‚
â”œâ”€ ğŸ¯ CLUSTERING (Finding groups)
â”‚  â”œâ”€ Know # of clusters? â†’ â­• **K-Means**
â”‚  â”œâ”€ Unknown # clusters? â†’ ğŸ” **DBSCAN**, **Hierarchical**
â”‚  â””â”€ High dimensions? â†’ ğŸ“‰ **PCA** + **K-Means**
â”‚
â””â”€ ğŸ” DIMENSIONALITY REDUCTION
   â”œâ”€ Linear patterns? â†’ ğŸ“‰ **PCA**
   â”œâ”€ Non-linear patterns? â†’ ğŸ—ºï¸ **t-SNE**, **UMAP**
   â””â”€ Feature selection? â†’ ğŸ¯ **SelectKBest**, **RFE**
```

---

## ğŸ” Detailed Algorithm Guide

### ğŸ“Š **Classification Problems**

#### **ğŸƒ For Fast Training & Prediction**
```python
# Best choices for speed
âœ… Naive Bayes         # Fastest, works well with text
âœ… Logistic Regression # Fast, interpretable
âœ… Linear SVM          # Fast for large datasets
```

#### **ğŸ¯ For High Accuracy**
```python
# Best performance (may be slower)
âœ… Random Forest       # Great all-rounder
âœ… Gradient Boosting   # Often highest accuracy
âœ… XGBoost/LightGBM   # State-of-the-art
âœ… Neural Networks     # For complex patterns
```

#### **ğŸ” For Interpretability**
```python
# Easiest to explain
âœ… Decision Tree       # Visual rules
âœ… Logistic Regression # Coefficient importance
âœ… Linear SVM          # Linear boundaries
âœ… Naive Bayes         # Feature probabilities
```

#### **ğŸ“ By Data Size**
```python
# Small datasets (< 10K)
âœ… SVM, KNN, Decision Tree, Naive Bayes

# Medium datasets (10K - 100K)  
âœ… Random Forest, Logistic Regression, SVM

# Large datasets (> 100K)
âœ… Logistic Regression, Linear SVM, SGD Classifier
```

---

### ğŸ“ˆ **Regression Problems**

#### **ğŸ“Š By Relationship Type**
```python
# Linear relationships
âœ… Linear Regression   # Simple linear
âœ… Ridge Regression    # With regularization
âœ… Lasso Regression    # With feature selection

# Non-linear relationships
âœ… Random Forest       # Handles non-linearity well
âœ… Support Vector Regression (SVR)
âœ… Gradient Boosting   # High accuracy
```

#### **ğŸ” By Data Characteristics**
```python
# High-dimensional data (many features)
âœ… Ridge/Lasso        # Regularization prevents overfitting
âœ… Elastic Net        # Combines Ridge + Lasso

# Small datasets
âœ… Linear Regression  # Simple, less prone to overfitting
âœ… Decision Tree      # Can capture non-linearity

# Large datasets
âœ… Linear Regression  # Scales well
âœ… Random Forest      # Parallel processing
```

---

### ğŸ¯ **Clustering Problems**

#### **ğŸ”¢ By Number of Clusters**
```python
# Known number of clusters
âœ… K-Means            # Fast, works well for spherical clusters
âœ… K-Medoids          # Robust to outliers

# Unknown number of clusters  
âœ… DBSCAN             # Finds arbitrary shapes, handles noise
âœ… Hierarchical       # Creates cluster tree
âœ… Gaussian Mixture   # Probabilistic clustering
```

#### **ğŸ“Š By Cluster Shape**
```python
# Spherical clusters
âœ… K-Means            # Assumes spherical clusters

# Arbitrary shapes
âœ… DBSCAN             # Any shape, handles noise
âœ… Spectral Clustering # Non-convex shapes

# Overlapping clusters
âœ… Gaussian Mixture   # Soft clustering
```

---

## ğŸš€ Algorithm Performance Comparison

### **ğŸƒ Speed Rankings (Training Time)**

| Rank | Algorithm | Speed | Use Case |
|------|-----------|-------|----------|
| ğŸ¥‡ | Naive Bayes | âš¡âš¡âš¡âš¡âš¡ | Text classification, small data |
| ğŸ¥ˆ | Linear Regression | âš¡âš¡âš¡âš¡ | Simple regression problems |
| ğŸ¥‰ | Logistic Regression | âš¡âš¡âš¡âš¡ | Binary classification |
| 4ï¸âƒ£ | Decision Tree | âš¡âš¡âš¡ | Interpretable models |
| 5ï¸âƒ£ | KNN | âš¡âš¡ | Simple classification (slow prediction) |
| 6ï¸âƒ£ | Random Forest | âš¡âš¡ | Balanced performance |
| 7ï¸âƒ£ | SVM | âš¡ | Small datasets |
| 8ï¸âƒ£ | Neural Networks | ğŸŒ | Complex patterns |

### **ğŸ¯ Accuracy Rankings (Typical Performance)**

| Rank | Algorithm | Accuracy | Trade-offs |
|------|-----------|----------|------------|
| ğŸ¥‡ | Gradient Boosting | ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ | Slow, prone to overfitting |
| ğŸ¥ˆ | Random Forest | ğŸ¯ğŸ¯ğŸ¯ğŸ¯ | Good all-rounder |
| ğŸ¥‰ | SVM | ğŸ¯ğŸ¯ğŸ¯ğŸ¯ | Slow on large data |
| 4ï¸âƒ£ | Neural Networks | ğŸ¯ğŸ¯ğŸ¯ | Need lots of data |
| 5ï¸âƒ£ | Logistic Regression | ğŸ¯ğŸ¯ğŸ¯ | Only linear boundaries |
| 6ï¸âƒ£ | KNN | ğŸ¯ğŸ¯ | Sensitive to irrelevant features |
| 7ï¸âƒ£ | Decision Tree | ğŸ¯ğŸ¯ | Prone to overfitting |
| 8ï¸âƒ£ | Naive Bayes | ğŸ¯ | Strong independence assumption |

---

## ğŸ¤” Decision Scenarios

### **ğŸ” "I need to explain my model to stakeholders"**
```python
# Choose interpretable algorithms
âœ… Decision Tree      # Visual rules
âœ… Linear Regression  # Clear coefficients  
âœ… Logistic Regression # Probability interpretation
âŒ Random Forest      # Black box
âŒ Neural Networks    # Very black box
```

### **âš¡ "I need results fast"**
```python
# Choose fast algorithms
âœ… Naive Bayes        # Fastest
âœ… Linear models      # Very fast
âœ… Decision Tree      # Fast training
âŒ SVM               # Slow on large data
âŒ Neural Networks   # Slow training
```

### **ğŸ¯ "I need the highest accuracy possible"**
```python
# Try ensemble methods
âœ… Random Forest      # Try first
âœ… Gradient Boosting  # Often best
âœ… XGBoost           # State-of-the-art
âœ… Neural Networks   # For complex data
```

### **ğŸ“Š "My dataset is small (< 1000 samples)"**
```python
# Avoid complex models
âœ… Naive Bayes       # Works with little data
âœ… Linear models     # Simple, less overfitting
âœ… KNN              # Non-parametric
âŒ Neural Networks  # Need lots of data
âŒ Deep ensembles   # Will overfit
```

### **ğŸš€ "My dataset is huge (> 1M samples)"**
```python
# Choose scalable algorithms
âœ… Linear models     # Scale well
âœ… SGD variants      # Online learning
âœ… Neural Networks   # Parallel processing
âŒ SVM              # Memory intensive
âŒ KNN              # Slow predictions
```

### **ğŸ” "I have many irrelevant features"**
```python
# Use feature selection or robust algorithms
âœ… Lasso Regression  # Built-in feature selection
âœ… Random Forest     # Feature importance
âœ… SVM              # Robust to irrelevant features
âŒ KNN              # Sensitive to irrelevant features
âŒ Naive Bayes      # Assumes all features relevant
```

---

## ğŸ› ï¸ Quick Implementation Guide

### **ğŸƒ 1-Minute Algorithm Test**
```python
# Quick performance comparison template
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression  
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'SVM': SVC(random_state=42)
}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5)
    print(f"{name}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

### **ğŸ¯ Algorithm Testing Pipeline**
```python
def quick_model_comparison(X, y, problem_type='classification'):
    """
    Quick comparison of multiple algorithms
    """
    if problem_type == 'classification':
        models = {
            'Logistic Regression': LogisticRegression(random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42),
            'SVM': SVC(random_state=42),
            'Naive Bayes': GaussianNB(),
        }
        scoring = 'accuracy'
    else:  # regression
        models = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(random_state=42),
            'SVR': SVR(),
        }
        scoring = 'r2'
    
    results = {}
    for name, model in models.items():
        scores = cross_val_score(model, X, y, cv=5, scoring=scoring)
        results[name] = {
            'mean': scores.mean(),
            'std': scores.std()
        }
    
    # Sort by performance
    sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)
    
    print("ğŸ† Model Performance Ranking:")
    for i, (name, score) in enumerate(sorted_results, 1):
        print(f"{i}. {name}: {score['mean']:.3f} (+/- {score['std'] * 2:.3f})")
    
    return sorted_results
```

---

## ğŸ¨ Visualization: When to Use Each Algorithm

```
                ğŸ“Š DATA SIZE
                     â†‘
         Large   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        (>100K)  â”‚ Logistic    â”‚ Neural
                 â”‚ Linear SVM  â”‚ Networks
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          
         Medium  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       (1K-100K) â”‚ Random      â”‚ SVM
                 â”‚ Forest      â”‚ KNN
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          
         Small   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        (<1K)    â”‚ Naive Bayes â”‚ Decision
                 â”‚ Linear      â”‚ Tree
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          
              Simple â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Complex
                    ğŸ” PROBLEM COMPLEXITY
```

---

## ğŸ”— Related Cheat Sheets

- **[Scikit-Learn Quick Reference](../02_python_sklearn/sklearn_quick_reference.md)** - Implementation syntax
- **[Classification Metrics](../04_model_evaluation/classification_metrics.md)** - How to evaluate results
- **[Hyperparameter Tuning](../07_troubleshooting/hyperparameter_tuning.md)** - Optimize your chosen algorithm
- **[Data Preprocessing](../05_data_preprocessing/preprocessing_pipeline.md)** - Prepare data for algorithms

---

## â— Key Takeaways

### **ğŸ¯ Golden Rules**
1. **Start simple** - Try linear models first
2. **Consider your constraints** - Speed vs accuracy vs interpretability  
3. **Test multiple algorithms** - Use cross-validation
4. **Ensemble methods** often win competitions
5. **Data quality** matters more than algorithm choice
6. **Feature engineering** can be more important than algorithm selection

### **ğŸš¨ Common Mistakes**
- âŒ Using complex models on small datasets
- âŒ Not scaling features for distance-based algorithms
- âŒ Choosing algorithms based on hype rather than fit
- âŒ Not considering prediction speed in production
- âŒ Ignoring interpretability requirements

---

**ğŸ  [Back to Cheat Sheets](../README.md)** | **ğŸ® [Try Interactive Tools](../../06_playground/)** | **ğŸ“š [Algorithm Details](../../02_algorithm_explanations/)**